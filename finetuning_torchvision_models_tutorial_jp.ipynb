{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetuning_torchvision_models_tutorial_jp.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxHWDLfCFH9z9Dl3M8Iw4J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "39a4d356978442beab4c912091995641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d7d8a0070dc4464871f421f7e96d7ce",
              "IPY_MODEL_d5267172c0ee45448e6244ade2fb66fa",
              "IPY_MODEL_d4f721a3bce641219040d0af3910d55f"
            ],
            "layout": "IPY_MODEL_5e433b2e663c49cb9ab0a93e4e15e727"
          }
        },
        "6d7d8a0070dc4464871f421f7e96d7ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62bcb5f0cc9e41d1959a64abfba8f77b",
            "placeholder": "​",
            "style": "IPY_MODEL_74b17bc8e5404a528d4929d3ba288a3f",
            "value": "100%"
          }
        },
        "d5267172c0ee45448e6244ade2fb66fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17b04b09979047f9bff5bfb4b5a68683",
            "max": 5010551,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ada1404736a74995b0faa2342fa0fdb6",
            "value": 5010551
          }
        },
        "d4f721a3bce641219040d0af3910d55f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8c59d5b9f3f4a11a9254eae1f43f905",
            "placeholder": "​",
            "style": "IPY_MODEL_0843ce939c95434cbdc81efd2f4acc8c",
            "value": " 4.78M/4.78M [00:00&lt;00:00, 13.6MB/s]"
          }
        },
        "5e433b2e663c49cb9ab0a93e4e15e727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62bcb5f0cc9e41d1959a64abfba8f77b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74b17bc8e5404a528d4929d3ba288a3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17b04b09979047f9bff5bfb4b5a68683": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ada1404736a74995b0faa2342fa0fdb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8c59d5b9f3f4a11a9254eae1f43f905": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0843ce939c95434cbdc81efd2f4acc8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/insilicomab/Pytorch_official_tutorials_JP/blob/main/finetuning_torchvision_models_tutorial_jp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html"
      ],
      "metadata": {
        "id": "s3Qf-Sxbnk4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "klvE6-CrxWRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "このチュートリアルでは、1000クラスのImagenetデータセットで事前学習されたtorchvisionモデルのファインチューニングと特徴抽出の方法をより深く見ていきます。このチュートリアルでは、いくつかの最新のCNNアーキテクチャの扱い方を深く見ていき、どのPyTorchモデルでもファインチューニングできるような直感を構築していきます。各モデルのアーキテクチャは異なるので、すべてのシナリオで機能する定型的な微調整コードはありません。むしろ、研究者は既存のアーキテクチャを見て、各モデルに対してカスタム調整を行う必要があります。\n",
        "  \n",
        "  \n",
        "このドキュメントでは、2種類の転移学習を行う。すなわち、ファインチューニングと特徴抽出である。ファインチューニングでは、事前に学習したモデルから始めて、新しいタスクに対してモデルの全てのパラメータを更新します。特徴抽出では、事前に学習したモデルを用いて、最終層の重みのみを更新し、そこから予測値を導きます。これは、事前学習されたCNNを固定された特徴抽出器として使い、出力層のみを変更することから特徴抽出と呼ばれています。転移学習に関するより詳しい技術的な情報はこちら（<https://cs231n.github.io/transfer-learning/>）とこちら（<https://ruder.io/transfer-learning/>）をご覧ください。\n",
        "\n",
        "\n",
        "一般に、どちらの転移学習法も同じようないくつかのステップを踏みます：\n",
        "\n",
        "*   学習済みモデルを初期化する\n",
        "*   新しいデータセットのクラス数と同じ数の出力を持つように、最終層を再形成する。\n",
        "*   最適化アルゴリズムに対して、学習中に更新したいパラメータを定義する\n",
        "*   学習ステップを実行\n",
        "\n"
      ],
      "metadata": {
        "id": "ExFJVMdbxRkb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUHlEtkLvkzs",
        "outputId": "e4afffb2-1cde-4cb3-de73-aa6606f33ce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.11.0+cu113\n",
            "Torchvision Version:  0.12.0+cu113\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function \n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inputs"
      ],
      "metadata": {
        "id": "snFGbh_szpq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここでは、実行のために変更するすべてのパラメータを示します。ここでは、ここからダウンロードできる *hymenoptera_data* データセットを使用します <https://download.pytorch.org/tutorial/hymenoptera_data.zip>。このデータセットにはハチとアリの2つのクラスがあり、独自のカスタムデータセットを書くのではなく、`ImageFolder` <https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder>データセットを使うことができるような構造になっています。データをダウンロードし、`data_dir`入力にデータセットのルートディレクトリを設定する。`model_name`入力は使いたいモデルの名前で、このリストから選択する必要がある。\n",
        "\n",
        "### `[resnet, alexnet, vgg, squeezenet, densenet, inception]`\n",
        "\n",
        "`num_classes` はデータセットのクラス数、`batch_size` は学習に使用するバッチサイズ、マシンの能力に応じて調整可能、`num_epochs` は実行したい学習エポックの数、`feature_extract` はファインチューニングか特徴抽出かを定義するブール型です。`feature_extract = False` の場合、モデルはファインチューニングされ、すべてのモデルパラメータが更新されます。`feature_extract = True`の場合、最後のレイヤーのパラメーターだけが更新され、他のパラメーターは固定されたままです。"
      ],
      "metadata": {
        "id": "xOw0p2Rpz4Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# トップレベルのデータディレクトリ\n",
        "# ここでは、ディレクトリの形式がImageFolderの構造に準拠するものとする。\n",
        "data_dir = \"./data/hymenoptera_data\"\n",
        "\n",
        "# 選べるモデル [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"squeezenet\"\n",
        "\n",
        "# データセットに含まれるクラス数\n",
        "num_classes = 2\n",
        "\n",
        "# 学習用バッチサイズ（メモリ容量により変更可）\n",
        "batch_size = 8\n",
        "\n",
        "# 学習するエポック数 \n",
        "num_epochs = 15\n",
        "\n",
        "# 特徴抽出のためのフラグ\n",
        "# Falseの場合、モデル全体を微調整し、\n",
        "# Trueの場合、リシェイプされたレイヤーパラメータのみを更新します\n",
        "feature_extract = True"
      ],
      "metadata": {
        "id": "lagf3dMrzsAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ヘルパー関数"
      ],
      "metadata": {
        "id": "31Z5YB2h2Aan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルを調整するコードを書く前に、いくつかのヘルパー関数を定義しておきましょう。  "
      ],
      "metadata": {
        "id": "N5ThEKiu65tB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# モデル学習・検証用コード\n"
      ],
      "metadata": {
        "id": "Hs5VXEIW67uw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`train_model` 関数は、与えられたモデルの学習と検証を処理します。入力として、PyTorchモデル、データロードの辞書、損失関数、オプティマイザ、学習と検証を行うエポック数、モデルがInceptionモデルであるかどうかのブール型フラグを受け取ります。*is_inception*フラグは、Inception v3モデルに対応するために使用されます。このアーキテクチャでは、補助出力を使用し、モデル全体の損失は、ここで説明するように、補助出力と最終出力の両方を尊重するからです。この関数は、指定されたエポック数で学習を行い、各エポックの後に完全な検証ステップを実行します。また，（検証精度の点で）最も性能の良いモデルを追跡し，トレーニングの終了時に最も性能の良いモデルを返します．各エポックの後、学習と検証の精度が表示されます。"
      ],
      "metadata": {
        "id": "DenNZuyw6-Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # 各エポックには学習と検証の段階がある\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # モデルを学習モードに設定する\n",
        "            else:\n",
        "                model.eval()   # モデルを評価モードに設定する\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # データを繰り返し処理する\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # パラメータの勾配をゼロにする\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # 学習モードのみの場合は履歴を追跡する\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # モデル出力の取得と損失額の算出\n",
        "                    # トレーニングでは補助出力があるため、inceptionの場合は特殊なケース。\n",
        "                    # トレーニングモードでは最終出力と補助出力の合計で損失を計算するが、\n",
        "                    # テストでは最終出力のみを考慮する\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize （学習モードのみ）\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # モデルをディープコピーする\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # 最適なモデルの重みをロードする\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "metadata": {
        "id": "YfCzk8br4dBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# モデルパラメータの.requests_grad属性の設定\n"
      ],
      "metadata": {
        "id": "Wkb2dd8z6rlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "このヘルパー関数は、特徴抽出を行う際に、モデルのパラメータの `.requires_grad` 属性を False に設定します。デフォルトでは、学習済みモデルをロードする際、全てのパラメータは `.requires_grad=True` となります。これは、ゼロから学習する場合や微調整を行う場合には問題ありません。しかし、特徴抽出を行い、新しく初期化されたレイヤーに対してのみ勾配を計算したい場合、他のすべてのパラメータは勾配を必要としないようにします。これは後々より意味を持つようになる。"
      ],
      "metadata": {
        "id": "dy89ohk27B-9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDGhtqd-vYSH"
      },
      "outputs": [],
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ネットワークの初期化と再形成"
      ],
      "metadata": {
        "id": "FfNniDas7jY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "さて、最も興味深い部分です。ここでは、各ネットワークのリシェイプを処理します。これは自動的な手順ではなく、各モデルに固有であることに注意されたい。CNNモデルの最終層、それはしばしばFC層であるが、データセットの出力クラスの数と同じ数のノードを持っていることを思い出してほしい。すべてのモデルはImagenetで事前学習されているので、それらはすべてサイズ1000の出力層を持っており、各クラスに1つのノードがあります。ここでのゴールは、最後の層を以前と同じ数の入力を持ち、かつデータセットのクラス数と同じ数の出力を持つように再形成することです。以下の節では、各モデルのアーキテクチャを個別に変更する方法について説明します。しかしその前に、微調整と特徴抽出の違いについて一つ重要なことがある。\n",
        "\n",
        "特徴抽出の場合、我々は最後の層のパラメータのみを更新したい、言い換えれば、整形する層のパラメータのみを更新したい。したがって、変更しないパラメータの勾配を計算する必要はないので、効率化のために `.requires_grad` 属性を `False` に設定します。これは重要なことで、デフォルトではこのアトリビュートは`True`に設定されています。そして、新しいレイヤーを初期化すると、デフォルトで新しいパラメーターは`.requests_grad=True`となり、新しいレイヤーのパラメーターだけが更新されます。微調整を行う際には、全ての`.required_grad`をデフォルトの`True`に設定したままにしておくことができます。\n",
        "\n",
        "最後に、inception_v3は入力サイズに(299,299)を要求していますが、他のモデルは(224,224)を要求していることに注意してください。"
      ],
      "metadata": {
        "id": "i9k68O-r7wZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resnet"
      ],
      "metadata": {
        "id": "HV1SbaeF8GvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resnetは、Deep Residual Learning for Image Recognitionという論文で紹介されました。Resnet18、Resnet34、Resnet50、Resnet101、Resnet152など、サイズの異なるいくつかのバリエーションがあり、いずれもtorchvisionモデルから入手可能である。ここでは、我々のデータセットが小さく、2クラスしかないため、Resnet18を使用します。モデルを印刷すると、以下のように最後の層が完全連結層であることがわかります。"
      ],
      "metadata": {
        "id": "2PABEBTO8Hy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `(fc): Linear(in_features=512, out_features=1000, bias=True)`"
      ],
      "metadata": {
        "id": "yZaMijah8TNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "したがって、`model.fc`を512個の入力特徴と2個の出力特徴を持つLinear層に再初期化する必要がある。"
      ],
      "metadata": {
        "id": "sZAM-XrY8akj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `model.fc = nn.Linear(512, num_classes)`"
      ],
      "metadata": {
        "id": "VZqN2Uch8g7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alexnet"
      ],
      "metadata": {
        "id": "nURvO9BW8nVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alexnetは論文ImageNet Classification with Deep Convolutional Neural Networksで紹介され、ImageNetデータセットで最初に大成功したCNNでした。モデルのアーキテクチャを表示すると、モデルの出力は分類器の6層目から来ることが分かります"
      ],
      "metadata": {
        "id": "CeUFbCCx8tR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(classifier): Sequential(\n",
        "    ...\n",
        "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
        " )\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "mWcgzACb8vZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我々のデータセットでモデルを使うために、このレイヤーを次のように再初期化する。"
      ],
      "metadata": {
        "id": "cabfgJhL85B5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `model.classifier[6] = nn.Linear(4096,num_classes)`"
      ],
      "metadata": {
        "id": "hP0Oynln87Dw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG"
      ],
      "metadata": {
        "id": "Dk5bUBD99RLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGGはVery Deep Convolutional Networks for Large-Scale Image Recognitionという論文で紹介されました。Torchvisionは様々な長さの8つのバージョンのVGGを提供しており、中にはバッチ正規化レイヤーを持つものもあります。ここでは、バッチ正規化のあるVGG-11を使用します。出力層はAlexnetと同様、すなわち"
      ],
      "metadata": {
        "id": "KiRxwVmz9Xdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(classifier): Sequential(\n",
        "    ...\n",
        "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
        " )\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "wd74jVxh9ZZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "そこで、同じ手法で出力層を修正します"
      ],
      "metadata": {
        "id": "719nvgr_9fbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `model.classifier[6] = nn.Linear(4096,num_classes)`"
      ],
      "metadata": {
        "id": "pU-TvMXQ9ijv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Squeezenet"
      ],
      "metadata": {
        "id": "y9LNdC2S9pgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Squeeznetのアーキテクチャは論文SqueezeNetで説明されています。50倍少ないパラメータと<0.5MBのモデルサイズでAlexNetレベルの精度を実現し、ここに示した他のどのモデルとも異なる出力構造を使用しています。Torchvisionには2つのバージョンのSqueezenetがあり、私たちはバージョン1.0を使用しています。出力は、分類器の第1層である1x1畳み込み層から得られます。"
      ],
      "metadata": {
        "id": "uMKnm0f99q2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(classifier): Sequential(\n",
        "    (0): Dropout(p=0.5)\n",
        "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
        "    (2): ReLU(inplace)\n",
        "    (3): AvgPool2d(kernel_size=13, stride=1, padding=0)\n",
        " )\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ffNjo-WT90Nr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ネットワークを修正するために、Conv2d層を再初期化し、深さ2の出力特徴マップを次のようにする。"
      ],
      "metadata": {
        "id": "9aFdsDLl98xc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ```model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))```"
      ],
      "metadata": {
        "id": "JYrxVSFY9_bg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Densenet"
      ],
      "metadata": {
        "id": "1qnV-_rO-OEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DensenetはDensely Connected Convolutional Networksという論文で紹介されました。Torchvisionには4種類のDensenetがありますが、ここではDensenet-121のみを使用します。出力層は1024個の入力特徴量を持つ線形層である。"
      ],
      "metadata": {
        "id": "6TOMPfGv-R3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `(classifier): Linear(in_features=1024, out_features=1000, bias=True)`"
      ],
      "metadata": {
        "id": "CtGufxmT-V2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ネットワークを再形成するために，分類器の線形層を以下のように再初期化する．"
      ],
      "metadata": {
        "id": "nX6lmDSX-hF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `model.classifier = nn.Linear(1024, num_classes)`"
      ],
      "metadata": {
        "id": "rKcp5mOJ-jix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inception v3"
      ],
      "metadata": {
        "id": "RAZTC0pi-tDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後に、Inception v3は、Rethinking the Inception Architecture for Computer Visionで初めて紹介されました。このネットワークは、トレーニング時に2つの出力レイヤーを持つことが特徴です。2つ目の出力は補助出力と呼ばれ、ネットワークのAuxLogits部分に含まれる。最終出力はネットワークの末尾にある線形層である。なお、テスト時には最終出力のみを考慮する。ロードされたモデルの補助出力と最終出力は次のように出力される。"
      ],
      "metadata": {
        "id": "wMcsjlp--wok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(AuxLogits): InceptionAux(\n",
        "    ...\n",
        "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
        " )\n",
        " ...\n",
        "(fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ZIEmWOo7-3vC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "このモデルをファインチューニングするためには、両方のレイヤーを再形成する必要があります。これは次のようにして行います。"
      ],
      "metadata": {
        "id": "AzOp3HDe-8ox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "model.AuxLogits.fc = nn.Linear(768, num_classes)\n",
        "model.fc = nn.Linear(2048, num_classes)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "FQ49wLKt_AvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "多くのモデルは似たような出力構造を持っていますが、それぞれを少し異なるように処理しなければならないことに注意してください。また、整形されたネットワークのプリントモデルアーキテクチャを確認し、出力特徴の数がデータセットのクラスの数と同じであることを確認します。"
      ],
      "metadata": {
        "id": "r5nqAlt__GVT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NaAho6avYSJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "39a4d356978442beab4c912091995641",
            "6d7d8a0070dc4464871f421f7e96d7ce",
            "d5267172c0ee45448e6244ade2fb66fa",
            "d4f721a3bce641219040d0af3910d55f",
            "5e433b2e663c49cb9ab0a93e4e15e727",
            "62bcb5f0cc9e41d1959a64abfba8f77b",
            "74b17bc8e5404a528d4929d3ba288a3f",
            "17b04b09979047f9bff5bfb4b5a68683",
            "ada1404736a74995b0faa2342fa0fdb6",
            "d8c59d5b9f3f4a11a9254eae1f43f905",
            "0843ce939c95434cbdc81efd2f4acc8c"
          ]
        },
        "outputId": "87534ad4-87da-488a-f79a-54970a2c24cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/4.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39a4d356978442beab4c912091995641"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SqueezeNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (3): Fire(\n",
            "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Fire(\n",
            "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Fire(\n",
            "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (7): Fire(\n",
            "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (8): Fire(\n",
            "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (9): Fire(\n",
            "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (10): Fire(\n",
            "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (12): Fire(\n",
            "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # このif文で設定されるこれらの変数を初期化します\n",
        "    # これらの変数はそれぞれモデルに依存します\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3 \n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # 補助出力\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # 最終出力\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "    \n",
        "    return model_ft, input_size\n",
        "\n",
        "# この実行のためのモデルを初期化する\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# インスタンス化したばかりのモデルを表示する\n",
        "print(model_ft)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# データのロード"
      ],
      "metadata": {
        "id": "oLkJSAC3AUu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "入力サイズがわかったので、データ変換、画像データセット、データローダを初期化することができます。ここで説明するように、モデルはハードコードされた正規化値で事前学習されていることに注意してください（<https://pytorch.org/docs/master/torchvision/models.html>）。"
      ],
      "metadata": {
        "id": "k0pM7IO1AgK5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5KqaJsYvYSL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "ff271337-488e-46dd-aaa3-1c0812c523d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Datasets and Dataloaders...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1d40676c30d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# 学習データセットと検証データセットの作成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mimage_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m# train dataloadersとvalidation dataloadersの作成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mdataloaders_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1d40676c30d1>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# 学習データセットと検証データセットの作成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mimage_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m# train dataloadersとvalidation dataloadersの作成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mdataloaders_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    143\u001b[0m     ) -> None:\n\u001b[1;32m    144\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \"\"\"\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/hymenoptera_data/train'"
          ]
        }
      ],
      "source": [
        "# 学習のためのデータの水増しと正規化\n",
        "# 検証のための正規化\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# 学習データセットと検証データセットの作成\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
        "# train dataloadersとvalidation dataloadersの作成\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "\n",
        "# GPUが利用可能かどうかを検出する\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# オプティマイザーの作成"
      ],
      "metadata": {
        "id": "I1-q2t5AC1Oc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデル構造が正しいので、ファインチューニングと特徴抽出のための最終ステップは、必要なパラメータのみを更新するオプティマイザを作成することです。事前学習されたモデルをロードした後、再形成する前に、`feature_extract=True`の場合、パラメータの`.requests_grad`属性を全て`False`に手動で設定したことを思い出してください。そして、再初期化されたレイヤーのパラメータは、デフォルトで` .requires_grad=True` になっています。これで、`.requests_grad=True`のパラメータはすべて最適化する必要があることがわかりました。次に、そのようなパラメータのリストを作成し、このリストを SGD アルゴリズムのコンストラクタに入力します。\n",
        "\n",
        "これを確認するために、出力されたパラメータを確認して学習します。ファインチューニングを行う場合、このリストは長くなり、すべてのモデルパラメータを含む必要があります。しかし、特徴抽出時には、このリストは短く、再形成された層の重みとバイアスのみを含むべきである。"
      ],
      "metadata": {
        "id": "J8QVn_3wLErB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwfCj7OnvYSM"
      },
      "outputs": [],
      "source": [
        "# モデルをGPUに送る\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# このランで最適化/更新するパラメータを収集します\n",
        "# ファインチューニングを行う場合は、すべてのパラメータを更新する\n",
        "# しかし、特徴抽出を行う場合は、先ほど初期化したパラメータ、\n",
        "# つまり requires_grad = True のパラメータのみを更新する\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# すべてのパラメータが最適化されていることを確認する\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# トレーニングおよび検証ステップを実行する"
      ],
      "metadata": {
        "id": "jl7vy79KMnQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後に、モデルの損失を設定し、設定したエポック数で学習と検証の機能を実行します。エポック数によっては、このステップにCPUの時間がかかることがあります。また、デフォルトの学習率はすべてのモデルに最適ではないので、最大の精度を得るためには、各モデルを個別に調整する必要があります。"
      ],
      "metadata": {
        "id": "Ha9PdxSbMsKs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpcSes1zvYSM"
      },
      "outputs": [],
      "source": [
        "# 損失関数を設定する\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 学習と評価\n",
        "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ゼロからトレーニングされたモデルとの比較"
      ],
      "metadata": {
        "id": "dX9kpZfNM977"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "面白いことに、転移学習を用いない場合、モデルがどのように学習するのかを見てみましょう。微調整と特徴抽出の性能はデータセットに大きく依存しますが、一般的にどちらの転移学習法も、ゼロから学習したモデルに対して、学習時間と全体的な精度の面で有利な結果を生み出します。"
      ],
      "metadata": {
        "id": "RlL9uryaNFi_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU16sOWUvYSM"
      },
      "outputs": [],
      "source": [
        "# この実行に使用されるモデルの事前トレーニングされていないバージョンを初期化します\n",
        "scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
        "scratch_model = scratch_model.to(device)\n",
        "scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
        "scratch_criterion = nn.CrossEntropyLoss()\n",
        "_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
        "\n",
        "# 転移学習法とゼロから学習したモデルについて、\n",
        "# 検証精度対学習エポック数の学習曲線をプロットしたもの\n",
        "ohist = []\n",
        "shist = []\n",
        "\n",
        "ohist = [h.cpu().numpy() for h in hist]\n",
        "shist = [h.cpu().numpy() for h in scratch_hist]\n",
        "\n",
        "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
        "plt.xlabel(\"Training Epochs\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
        "plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
        "plt.ylim((0,1.))\n",
        "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 最終的な考えと次に進むべき場所"
      ],
      "metadata": {
        "id": "O-ZPnu-QNuja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "他のモデルも動かしてみて、精度がどの程度になるかを見てみましょう。また、バックワードパスでは勾配の大部分を計算する必要がないため、特徴抽出にかかる時間が短くなっていることに注目してください。ここから先はいろいろなことが考えられます。あなたにもできるはずです。\n",
        "\n",
        "\n",
        "\n",
        "*   このコードをより難しいデータセットで実行し，転移学習の利点をさらに見てみましょう． \n",
        "*   ここで説明した方法を使って、転移学習により、おそらく新しいドメイン（NLP、オーディオなど）の別のモデルを更新する。\n",
        "*   モデルが完成したら、ONNXモデルとしてエクスポートしたり、ハイブリッドフロントエンドを使用してトレースすることで、より高速で最適化されたモデルを作成することができます。\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ggdfShrwN2ja"
      }
    }
  ]
}